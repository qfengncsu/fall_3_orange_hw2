rm(list=ls()); gc()
library(dplyr)
library(splines)
library(mclust)
library(factoextra)
library(tidyverse)
load("/Users/qing/Desktop/Clustering/Project/final_data.Rdata")
times <- seq(1,295)/100 # Observations in 1/100th of a second
X <- bs(times,intercept=TRUE,df=60) #create a spline to
#model the data
betas <- matrix(0,ncol=60,nrow = 6792)
###########################################################
# run a linear regression on each data set
# here I am manipulating my data you I can cluster
###########################################################
for (ii in 1:6792){
temp <- lm(as.numeric(final_data[ii,6:300])~X-1) #-1 removes the natural intercept
betas[ii,]  <- coefficients(temp)
}
cdata <- cbind(final_data[,1:5],betas)
cdata$AGE <- as.numeric(cdata$AGE)
cdata$EVER_SMOKE <- as.numeric(cdata$EVER_SMOKE)
cdata$ASTHMA <- as.numeric(cdata$ASTHMA)
cdata$POVERTY_RATIO <- as.numeric(cdata$POVERTY_RATIO)
##Part One###
#a) Perform a principal components analysis on columns 2 through 65. List the standard
#deviations for the first 5 components.
set.seed(12345)
pca_result <- princomp(cdata[,2:65])
pca_result$sdev[1:5]
set.seed(12345)
fviz_nbclust(pca_result$scores, kmeans, method = "wss",k.max=20) ##between 2 and 4 approx.
View(pca_result)
fviz_nbclust(pca_result$scores, kmeans, method = "silhouette",k.max=20)  ##2 would be the best
set.seed(12345)
kmean_4 <- kmeans(pca_result$scores,4,nstart=25)
cdata$clust <- kmean_4$cluster
aux_1 <- aggregate(cdata[,6:65], by = list(cdata$clust), mean, na.rm=TRUE)
aux_1 <- as.matrix(aux_1)
class(aux_1)
bl1 <- matrix(aux_1[1,-1],60,1)
plot(times,as.matrix(X)%*%bl1,ylab="ML",xlab = "Time",type = 'l',lwd=2,col=1)#,ylim=c(-5,5))
bl2 <- matrix(aux_1[2,-1],60,1)  #red
lines(times,X%*%bl2,lwd=2,col=2)
bl3 <- matrix(aux_1[3,-1],60,1) #green
lines(times,X%*%bl3,lwd=2,col=3)
bl4 <- matrix(aux_1[4,-1],60,1) #blue
lines(times,X%*%bl4,lwd=2,col=4)
legend(1.8, 70, legend=c("Cluster 1", "Cluster 2", "Cluster 3","Cluster 4"),
col=c("black","red","green" ,"blue"),lty=1,  cex=1 , y.intersp = 0.6)
bl3 <- matrix(aux_1[3,-1],60,1) #green
plot(times,X%*%bl3,ylab="ML",xlab = "Time",type = 'l',lwd=2,col="green")#,ylim=c(-5,5))
legend(1.8, 20, legend=c("Cluster 3"),
col=c("green"),lty=1,  cex=1 , y.intersp = 0.6)
###
# Understanding the clusters
###
#
clusters <- list()
for( ii in 1:4){
clusters[[ii]] <-  cdata %>% filter(clust == ii)
}
#
# Finding the means of each cluster to "Name them"
x_means <- cbind(colMeans(cdata))
y <- x_means
for (ii in 1:4) {
x_means <- cbind(x_means,colMeans(clusters[[ii]]))
}
colnames(x_means) <- c("Overall","Cluster1","Cluster2","Cluster3","Cluster4")
print(x_means[1:5,])
sum(bl1 * 0.01) #black  -largest lung capacity  - Cluster 1
sum(bl2 * 0.01) #red    -least lung capacity  - Cluster 2
sum(bl3 * 0.01) #green
sum(bl4 * 0.01) #blue
##Jacob's method:
sfun <- splinefun(times,X%*%bl1)
integrate(sfun,min(times),max(times)) # Longest lung capacity  - Cluster 1
sfun <- splinefun(times,X%*%bl2)
integrate(sfun,min(times),max(times))  #least lung capacity  - Cluster 2
sfun <- splinefun(times,X%*%bl3)
integrate(sfun,min(times),max(times))
sfun <- splinefun(times,X%*%bl4)
integrate(sfun,min(times),max(times))
print(x_means[1:5,-4])
plot(times,as.matrix(X)%*%bl1,ylab="ML",xlab = "Time",type = 'l',lwd=2,col="black", ylim = c(0,95))#red
lines(times,X%*%bl2,lwd=2,col="red") #green
lines(times,X%*%bl4,lwd=2,col="blue") #blue
legend(1.8, 70, legend=c("Cluster 1", "Cluster 2","Cluster 4"),
col=c("black","red" ,"blue"),lty=1,  cex=1 , y.intersp = 0.6)
library(mclust)
set.seed(12345)
clustBIC <-mclustBIC(cdata[,10:20], G = 1:20, modelNames = "VVV")
plot(clustBIC)
summary(clustBIC)
set.seed(12345)
clustBIC_final <-Mclust(cdata[,10:20], G = 6, modelNames = "VVV")
cdata$class <- as.factor(clustBIC_final$classification)
aux_2 <- aggregate(cdata[,6:65], by = list(cdata$class), mean, na.rm=TRUE)
aux_2 <- as.matrix(aux_2)
class(aux_2)
bl2_1 <- matrix(as.numeric(aux_2[1,-1]),60,1) #black
bl2_2 <- matrix(as.numeric(aux_2[2,-1]),60,1) #red
bl2_3 <- matrix(as.numeric(aux_2[3,-1]),60,1) #green
bl2_4 <- matrix(as.numeric(aux_2[4,-1]),60,1) #blue
bl2_5 <- matrix(as.numeric(aux_2[5,-1]),60,1) #orange
bl2_6 <- matrix(as.numeric(aux_2[6,-1]),60,1) #purple
plot(times,as.matrix(X)%*%bl2_1,ylab="ML",xlab = "Time",type = 'l',lwd=2,col=1, ylim = c(0,90))#,ylim=c(-5,5))
lines(times,X%*%bl2_2,lwd=2,col=2)
lines(times,X%*%bl2_3,lwd=2,col=3)
lines(times,X%*%bl2_4,lwd=2,col=4)
lines(times,X%*%bl2_5,lwd=2,col="orange")
lines(times,X%*%bl2_6,lwd=2,col="purple")
legend(1.8, 70, legend=c("Cluster 1", "Cluster 2", "Cluster 3","Cluster 4", "Cluster 5","Cluster 6"),
col=c("black","red","green" ,"blue","orange","purple"),lty=1,  cex=1 , y.intersp = 0.6)
clustBIC_final$classification
set.seed(12345)
clustBIC_final <-Mclust(cdata[,10:20], G = 6, modelNames = "VVV")
cdata$class <- as.factor(clustBIC_final$classification)
aux_2 <- aggregate(cdata[,6:65], by = list(cdata$class), mean, na.rm=TRUE)
aux_2 <- as.matrix(aux_2)
class(aux_2)
bl2_1 <- matrix(as.numeric(aux_2[1,-1]),60,1) #black
bl2_2 <- matrix(as.numeric(aux_2[2,-1]),60,1) #red
bl2_3 <- matrix(as.numeric(aux_2[3,-1]),60,1) #green
bl2_4 <- matrix(as.numeric(aux_2[4,-1]),60,1) #blue
bl2_5 <- matrix(as.numeric(aux_2[5,-1]),60,1) #orange
bl2_6 <- matrix(as.numeric(aux_2[6,-1]),60,1) #purple
plot(times,as.matrix(X)%*%bl2_1,ylab="ML",xlab = "Time",type = 'l',lwd=2,col=1, ylim = c(0,90))#,ylim=c(-5,5))
lines(times,X%*%bl2_2,lwd=2,col=2)
lines(times,X%*%bl2_3,lwd=2,col=3)
lines(times,X%*%bl2_4,lwd=2,col=4)
lines(times,X%*%bl2_5,lwd=2,col="orange")
lines(times,X%*%bl2_6,lwd=2,col="purple")
legend(1.8, 70, legend=c("Cluster 1", "Cluster 2", "Cluster 3","Cluster 4", "Cluster 5","Cluster 6"),
col=c("black","red","green" ,"blue","orange","purple"),lty=1,  cex=1 , y.intersp = 0.6)
View(aux_2)
View(bl2_1)
View(bl2_2)
