library(tidytext)
library(stringr)
library(dplyr)
library(text2vec)
library(readr)

nrc_total <- get_sentiments("afinn")
# the more positive word, the more positive number;
# the more negative word, the more negative number.

#Load the dataset and group the reviews by the listing id 
#keep all listings having more than 4 reviews 
reviews <- read_csv("/Users/qing/Desktop/Clustering/Project/boston-airbnb-open-data/reviews.csv")

rv <- reviews %>% 
  group_by(listing_id) %>%
  count(listing_id, sort = TRUE) %>% 
  filter( n >= 4) %>% 
  select(-"n")
###rv: 2023*1 - listing id having >= 4 reviews

#break up the dataset's listing into individual words
#join this dataset with rv, keeping only those listing IDs that have 4 or more
# reviews remove all of the na values
new_reviews <- reviews %>%
  group_by(listing_id) %>%
  unnest_tokens(word, comments)  %>%
  right_join(rv,by="listing_id") %>% filter(!is.na(word)) %>%
  left_join(nrc_total,by="word") %>% filter(!is.na(score))
###new_reviews: 350066*7 - listing id + word + score

# find the total score in the listing
score <- new_reviews %>% 
         group_by(listing_id) %>% 
         mutate(sscore = sum(score)) %>% 
         distinct(listing_id,sscore)
###score: 2023*2 - listing id + sum of score

# find the number of words in the listing
nwords<- new_reviews %>% 
         group_by(listing_id) %>% 
         count(listing_id) 
###nwords: 2023*2 - listing id + n (number of words)

complete <- nwords %>% left_join(score,"listing_id") %>% mutate(avg = sscore/n)
###complete: 2023*4 - listing id + sum of score + number of score + average score

#average positivity/negativity score
hist(complete$avg)
# the story is meaningless. 

complete$avg <- scale(complete$avg) #standardize the score
# mean is zero, everything above zero is above average.
hist(complete$avg)

##############load listings.csv and Google Map's Boston data #############
library(lubridate)
library(ggplot2)
library(ggmap)
library(data.table)
library(ggrepel)

listings <- read_csv("/Users/qing/Desktop/Clustering/Project/boston-airbnb-open-data/listings.csv")
load("/Users/qing/Desktop/Clustering/Project/boston.RData")

listing_k <-data.frame(listing_id = listings$id, lat = listings$latitude, lon = listings$longitude)
### listings_k: 3585*3 - listing id, latitude, longitude

combined <- complete %>% left_join(listing_k,"listing_id")
###combined: 2023*6 - 
###listing id + sum of score + number of score + average score+ lon + lat

combined$std.lat <- scale(combined$lat)
combined$std.lon <- scale(combined$lon)


### Cluster 1 : by sentiment
toC<- cbind(combined$avg,combined$std.lat,combined$std.lon)
colnames(toC) <- c("avg_score", "std_lat", "std_lon")

clusters.c <- hclust(dist(toC),method="complete")
#clusters.s <- hclust(dist(toC), method="single")
#clusters.a <- hclust(dist(toC), method="average")

plot(clusters.c)
#plot(clusters.s)
#plot(clusters.a)

combined$clus <- cutree(clusters.c,3) #it looks like 3 clusters is reasonable

library(ggmap)
clu1 <- combined %>% filter(clus == 1) # 519 obs
clu2 <- combined %>% filter(clus == 2) # 1464 obs
clu3 <- combined %>% filter(clus == 3) # 40 obs
#clu4 <- combined %>% filter(clus == 4)
#clu5 <- combined %>% filter(clus == 5)
#clu6 <- combined %>% filter(clus == 6)

################################
# plot threeQuestions  interesting clusters
ggmap(map, fullpage = TRUE) +
  geom_point(data = clu1, aes(x = lon, y = lat), color = '#F3E96B', size = 2)+
  geom_point(data = clu2, aes(x = lon, y = lat), color = '#F28A30', size = 2)+
  geom_point(data = clu3, aes(x = lon, y = lat), color = '#563838', size = 2)
  

### yellow - cluster 1         0.1121462
### orange - cluster 2         0.04974964
### dark brown - cluster 3     -3.275934
#mean(clu1$avg)

################################################
##### add more columns to combined datafram #####
attractions <- read_csv("/Users/qing/Desktop/Clustering/Project/boston-airbnb-open-data/Attractions.csv")

# standardize longitutude & latitude
attractions$std_lat_attraction <- scale(attractions$Lat)
attractions$std_lon_attraction <- scale(attractions$Lon)

library(tidyr)
library(geosphere)

combined_attr <- crossing(combined, attractions)
# combine combined with attractions. 2023*10=20230 rows


# calculate distance between each property and each attraction
combined_attr$distance <- distm(cbind(combined_attr$std.lon, combined_attr$std.lat), 
          cbind(combined_attr$std_lon_attraction, combined_attr$std_lat_attraction), 
          fun = distHaversine)

combined_attr <- combined_attr %>% 
  group_by(listing_id) %>% 
  mutate(avg_distance = mean(distance))%>%
  distinct(listing_id, avg_distance)%>%
  right_join(combined,by="listing_id")
### combined_attr: 2023*9 

### Cluster 2 : by average distance

#standardize avg_distance and check histogram first
combined_attr$avg_distance <- scale(combined_attr$avg_distance)
hist(combined_attr$avg_distance)

toD<- cbind(combined_attr$avg_distance,combined_attr$std.lat,combined_attr$std.lon)
colnames(toD) <- c("avg_distance", "std_lat", "std_lon")

clusters.c2 <- hclust(dist(toD),method="complete")

plot(clusters.c2)

combined_attr$clus <- cutree(clusters.c2,5) #it looks like 4 clusters is reasonable

library(ggmap)
clu1 <- combined_attr %>% filter(clus == 1) # 201 obs
clu2 <- combined_attr %>% filter(clus == 2) # 1277 obs
clu3 <- combined_attr %>% filter(clus == 3) # 133 obs
clu4 <- combined_attr %>% filter(clus == 4) # 118 obs
clu5 <- combined_attr %>% filter(clus == 5) # 294 obs

ggmap(map, fullpage = TRUE) +
  geom_point(data = clu1, aes(x = lon, y = lat), color = '#F3E96B', size = 2)+
  geom_point(data = clu2, aes(x = lon, y = lat), color = '#F28A30', size = 2)+
  geom_point(data = clu3, aes(x = lon, y = lat), color = '#563838', size = 2)+
  geom_point(data = clu4, aes(x = lon, y = lat), color = 'blue', size = 2) +
  geom_point(data = clu5, aes(x = lon, y = lat), color = 'green', size = 2)


mean(clu5$avg_distance)


########### calculate price/bed
listings$price <- as.numeric(gsub('[$,]', '', listings$price))
listings$price_per_bed <- listings$price/listings$beds

# extract id and price_per_bed from listings
listing <- cbind(listings$id, listings$price_per_bed)
colnames(listing) <- c("listing_id", "price_per_bed")
listing <- as.data.frame(listing)

combined_attr_price <- combined_attr %>% 
  group_by(listing_id) %>% 
  left_join(listing,by="listing_id")

final_matrix <- cbind(combined_attr_price$listing_id, 
                      combined_attr_price$std.lat,
                      combined_attr_price$std.lon,
                      combined_attr_price$avg,
                      combined_attr_price$std_avg_distance,
                      combined_attr_price$price_per_bed)
colnames(final_matrix) <- c("listing_id", "listing_std_lat", "listing_std_lon",
                            "avg_score", "std_avg_distance", "price_per_bed")

#### what variables should we standardize???? to confirm